% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/impute_data_validate.R
\name{impute_data_validate}
\alias{impute_data_validate}
\title{Missing data imputation with various methods}
\usage{
impute_data_validate(X, scale = T, spike.in = 0.01, n.iter = 10,
  sel_method = c(1:16))
}
\arguments{
\item{X}{Dataframe - the original data that contains missing values.}

\item{scale}{Boolean with default TRUE. Scaling will scale and center all variables to mean=0 and standard deviation=1. This is strongly suggested for all PCA-based methods, and for the sake of comparison (and in the case when all methods are run), for the other methods too. Please note, however, that some methods (e.g. pcaMethods NLPCA, missForest, etc.) are equipped to handle non-linear data. In these cases scaling is up to the user.}

\item{spike.in}{Numeric value with 0.01 (or 1\%) as default. This parameter controls the additional missing data spike-in.}

\item{n.iter}{Number of iterations to perform with default 10. This will only affect the probabilistic methods that allow for a multiple imputation framwork. The rest of the methods (if specified to run) will only generate 1 imputed dataframe.}

\item{sel_method}{Numeric vector that specifies which methods to run. Default is all methods (1-16), but any combinations, including selecting a single method, are allowed. \tabular{ll}{
1 \tab random replacement\cr
2 \tab median imputation\cr
3 \tab mean imputation\cr
4 \tab missMDA Regularized\cr
5 \tab missMDA EM\cr
6 \tab pcaMethods PPCA\cr
7 \tab pcaMethods svdImpute\cr
8 \tab pcaMethods BPCA\cr
9 \tab pcaMethods NIPALS\cr
10 \tab pcaMethods NLPCA\cr
11 \tab mice mixed\cr
12 \tab mi Bayesian\cr
13 \tab Amelia II\cr
14 \tab missForest\cr
15 \tab Hmisc aregImpute\cr
16 \tab VIM kNN\cr
}}
}
\value{
A nested list of imputed datasets. In case only a subset of methods was selected the not-selected list elements will be empty.
\item{random_replacement}{Imputed dataset using random replacement}
\item{mean_imputation}{Imputed dataset using mean imputation}
\item{median_imputation}{Imputed dataset using median imputation}
\item{missMDA_reg_imputation}{Imputed dataset using the missMDA regularized imputation algorithm}
\item{missMDA_EM_imputation}{Imputed dataset using the missMDA EM imputation algorithm}
\item{pcaMethods_PPCA_imputation}{Imputed dataset using the pcaMethods PPCA imputation algorithm}
\item{pcaMethods_svdImpute_imputation}{Imputed dataset using the pcaMethods svdImpute imputation algorithm}
\item{pcaMethods_BPCA_imputation}{Imputed dataset using the pcaMethods BPCA imputation algorithm}
\item{pcaMethods_Nipals_imputation}{Imputed dataset using the pcaMethods Nipals imputation algorithm}
\item{pcaMethods_NLPCA_imputation}{Imputed dataset using the pcaMethods NLPCA imputation algorithm}
\item{mice_mixed_imputation}{Imputed dataset using the mice mixed imputation algorithm}
\item{mi_Bayesian_imputation}{Imputed dataset using the mi Bayesian imputation algorithm}
\item{ameliaII_imputation}{Imputed dataset using the Amelia2 imputation algorithm replacement}
\item{missForest_imputation}{Imputed dataset using the missForest imputation algorithm replacement}
\item{Hmisc_aregImpute_imputation}{Imputed dataset using the Hmisc aregImpute imputation algorithm}
\item{VIM_kNN_imputation}{Imputed dataset using the VIM kNN imputation algorithm replacement}
}
\description{
\code{\link{impute_data_vaidate}} imputes a dataframe with missing values with selected algorithm(s) using a validation framwork
}
\details{
This function assumes that the user has performed simulations using the \code{\link{impute_simulated}} function and arrived to
some conclusions regarding which functions would be the best performing on their datasets. This function offers a convenient
way to validate this on the original dataset. This function differs from the \code{\link{impute_data}} function as here a very small
amount of extra missing data is spike in and the imputation performance is tested on this small fraction, whereas the \code{\link{impute_data}}
function does not allow for this validation step. Therefore, it is suggested that the \code{\link{impute_data_vaidate}} is used before undertaking
the data imputation using \code{\link{impute_data}}. The additional missing values (1\% as default) will be indexed and RMSE will be calculated
using the original datapoints and the indexed imputed values. The spike.in parameter spikes in additional missing values in a random fashion
(the assumption here is that a very small amount of extra missing values will not distort the missingness pattern). The user can specify whether
all methods or only a subset of methods will be included in this validation step.
}
\examples{
\dontrun{
#running 10 iterations of all algoritms (that allow for multiple imputation) and one copy of those that do not allow for multiple imputations with 1\% spike-in for validation
impute_data_validate(df, scale = T, spike.in = 0.01, n.iter = 10, sel_method = c(1:16))
#running 5 iterations of pcaMethods NLPCA and missForest (e.g. these two were the best performing algorithms in simulations) on a non-scaled dataframe with 1\% spike-in for validation
res <- impute_data_validate(df, scale = F, spike.in = 0.01, n.iter = 5, sel_method = c(10,16))
}

}
