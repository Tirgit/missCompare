---
title: "Introduction to missCompare"
author: "Tibor V. Varga"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{missCompare}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\VignetteDepends{devtools}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
devtools::load_all()
```


If you work with data, you have almost certainly encountered issues regarding missing data. The most commonly used statistical methods often only work with complete sets of observations and it can be a huge pain when you realize that you can only utilize a fraction of your data because of missing datapoints here and there. Indeed, just 5\% random missing data in a huge dataset can dramatically reduce the number of complete observations to as low as 25\% of the original number of observations! Scary! Thankfully, there are numerous algorithms that can help you tackle this issue, but often, the most commonly used methods, such as mean and median imputation, offer suboptimal solutions and sophisticated algorithms often need a lot of consideration and background knowledge. The authors wrote the missCompare package to help you through your missing data problem. The initial hypothesis of the authors was that **datasets with different parameters (size, distributions, correlations, etc.) will benefit differently from various missing data imputation algorithms**.

This tutorial will navigate you through the missCompare package and its functions. So... let's start.

First, let's begin by installing and loading the missCompare package!

```{r eval = FALSE}
install.packages("missCompare")
library(missCompare)
```


## Description of data: *clindata_full* and *clindata_miss*

To walk you through the missCompare missing data imputation pipeline, the authors created
the `missCompare::clindata_full` and `missCompare::clindata_miss` dataframes. As the names suggest, while clindata_miss contains missing data, clindata_full does not. You can load them into your workspace by:

```{r eval = TRUE}
data("clindata_full")
data("clindata_miss")
```

The data is created by the authors with the purpose of generating a dataset that resembles a real-life
dataset with variables generally available in realistic clinicial epidemiology datasets. The datasets
have "believable" variable means, distributions, correlations and in clindata_miss, realistic
missing data patterns. You can find further information on these datasets and the variables within
using `?clindata_full` and `?clindata_miss`.


## The missCompare pipeline

The missCompare pipeline is comprised of the following steps:

1. **Cleaning your data** using `missCompare::clean()`
2. **Extracting information** on dimensions, missingness, correlations and variables, plotting
missing data using `missCompare::get_data()`
3. **Imputation - simulated data**:
  + simulating full data with no missingness using metadata from the previous step (resembling your original data) using `missCompare::simulated()`
  + spiking in missing data in distinct missing data patterns using `missCompare::all_patterns()`. These patterns are:
    + missing completely at random (MCAR) - `missCompare::MCAR()` - missing data occurence random
    + missing at random (MAR) - `missCompare::MAR()` - missing data occurence correlates with other variables' values (univariate solution in missCompare)
    + missing not at random (MNAR) - `missCompare::MNAR()` - missing data occurence correlates with variables' own values
    + missing in assumed pattern (MAP) - `missCompare::MAP()` - a combination of the previous three, where the user can define a pattern per variable
  + imputing missing data, obtaining imputation accuracy (root mean squared errors - RMSE) per method and plotting results using `missCompare::impute_simulated()`
4. **Imputation - validation** - After the previous step, you will have a general idea on what are the best performing algorithms for your data structure (size, degree of correlation between variables). With `missCompare::impute_data_validate()` you can use your original data and spike in a tiny fraction of missing data, impute and check imputation accuracy on that tiny fraction and validate that the best performing algorithms from the previous step are indeed good to work with.
5. **Imputating your data** - Here you can impute your original data with your chosen algorithm(s) using `missCompare::impute_data()`.
6. **Post imputation diagnostics** will give an informative assessment on how the imputation changed your data structure (e.g. variable means, distributions, correlations). The function here is `missCompare::post_imp_diag()`.

Now let's look into these, step by step!

## Cleaning and getting to know the data

The first step before starting anything imputation related is getting to know your data and cleaning it if necessary. This is not an "automated" process, nor should it be. You always need to explore your data before missing data imputation - it is entirely up to you what to keep and what to drop from your dataset before the imputation process. After the cleaning steps, missCompare will essentially reconstruct your dataset with no missing data, and in order to achieve this, your dataframe cannot have variables of type character or factor (in a later stage you will be able to use your original factor variables, but to extract metadata at this step, a conversion is necessary). 

Let's take clindata_miss and run `missCompare::clean()`. In one step, you can achieve multiple cleaning steps:

```{r eval = TRUE} 
cleaned <- missCompare::clean(clindata_miss,
                              var_removal_threshold = 0.5, 
                              ind_removal_threshold = 0.8,
                              missingness_coding = -9)
```

* You can convert all factor variables (sex and education, in this case) to numeric. The function will alert you with a message: **"Variable(s) sex, education converted to numeric."**
* You can remove variables exceeding a set threshold (default is 50\%, but this is up to you, you can set it lower or higher). In other words, you can discard those variables where lots of data are missing. Such variables can only be imputed with low accuracy, so the authors suggest that you remove variables with high missingness fraction, but there is no recipe for this, e.g. you will find no guidelines on whether 50\% is too high or too low, the threshold will be YOUR decision. In case there is a variable removed here, the function will alert you with a message: **Variable(s) PPG removed due to exceeding the pre-defined removal threshold (>50%) for missingness.**
* Similarly, you can remove individuals with a high percentage of missing values (default is 100\%, but this is up to you, you can set it lower). In other words, you can discard those observations that have only missing datapoints (these will be totally uninformative for imputation), or those with lots of missing data points. Again, there are no guidelines here, the threshold is ultimately YOUR decision. In this case, we set this threshold to 0.8, and the function alerted us with a message: **1 individual(s) removed due to exceeding the pre-defined removal threshold (>80%) for missingness.**
* In some datasets, missing values can be coded in a weird way. In clindata_miss, we emulated a real-life scenario where lipid values were set to **-9** when measured below the sensitivity of a hypothetical machine. The cleaning function will let you convert pre-defined values to **NAs**.

The cleaning function outputs the results to an R object called `cleaned`. This is list that contains one element, `cleaned$Dataframe_clean`, the cleaned dataframe. In the next step, where you extract important metadata from your dataframe, you will continue working with this object.

You will get to know important details of your cleaned dataframe using the following line:

```{r eval = TRUE} 
metadata <- missCompare::get_data(cleaned$Dataframe_clean,
                                  matrixplot_sort = T,
                                  plot_transform = T)
```

If there are factor variables in the data, a warning will appear: **Warning! Variable(s) sex, education is/are not numeric. Convert this/these variables to numeric using missCompare::clean() and repeat the missCompare::get_data() function until no warnings are shown.**

As we cleaned clindata_miss in the previous step, the function should run without any warnings and errors. The output object, `metadata` is a list of the following elements.

* `metadata$Complete_cases` - the number of complete observations with no missing data. In this case, there are **1349 observations from the original 2500** with no missing data. Note, that with only ~7\% missing data, the number of complete observations is dramatically decreased to around half of the original data. As many methods (e.g. regression, principal component analysis, etc.) require complete sets of observations, this decrease in complete observations presents a very strong case for missing data imputation.
* `metadata$Rows` and `metadata$Columns` give you the dimensions of the dataframe, **2499 rows and 11 columns**, in our case.
* `metadata$Corr_matrix` - gives you the **correlation matrix of all variables** in the dataframe. The function assesses pairwise Pearson correlation coefficients using complete sets of pairs between any two variables. For example there are 2312 complete BMI-waist pairs (`sum(!is.na(cleaned$Dataframe_clean$BMI) & !is.na(cleaned$Dataframe_clean$waist))`) - in this case correlation will be calculated based on these 2312 value pairs, even though the number of complete observations (no NAs for any variables) is 1349. This way all of the data will be utilized to obtain the most accurate correlation coefficients.
* `metadata$Fraction_missingness` and `metadata$Fraction_missingness_per_variable` return the the total fraction of missingness in the dataframe and the fraction of missingness per variable, respectively. In our case, the **total fraction of missingess is 6.9\%** after removing one variable, PPG with >50\% missing. Inspecting the fraction of missingness per variable it is also apparent that the true missing data fraction of TG and HDL-C after converting -9s to NAs are 10.6\% and 13.7\%, respectively.
* `metadata$Total_NA` and `metadata$NA_per_variable` return the total number of missing values in the dataframe and per variable.
* `metadata$MD_Pattern` - The missing data pattern is calculated using mice::md.pattern (`?mice::md.pattern`).
* `metadata$Vars_above_half` - In case there are variables exceeding 50\% missingness, this object will list these variables and the function will output a warning: **Warning! Missingness exceeds 50% for variable(s) PPG. Although the pipeline will function with variables with high missingness, consider excluding these variables using missCompare::clean() and repeating function until no warnings are shown.**
* `metadata$Matrix_plot` - Matrix plot (ggplot2 object) visualizing **missing data distribution**. Missing data are represented by gray color, while data is colored by value. There are options to sort the dataframe by missingness or leave the dataframe unsorted. This plot (hopefully) gives you a visual description of how data is missing together and what fraction of the data is missing.     
```{r echo = FALSE, fig.width=6, fig.height=3} 
metadata$Matrix_plot
```
* `metadata$Cluster_plot` - Dendrogram (ggplot2 object) visualizing the **hierarchical clustering of missing data**. On the plot, the clades (bifurcations) closer to Height 0 (the bottom of the plot) represent closer relationships in terms of missing data - i.e. those variables in one group are more likely to be missing together compared to the rest. In our case, blood pressures SBP and DPB are missing together often and so as the three lipid traits, TC, TG and HLD-C. With respect to the other variables, waist and BMI have a closer relationship compared to the rest.    
```{r echo = FALSE, fig.width=6, fig.height=3} 
metadata$Cluster_plot
```


## Simulating a complete dataset and assessing algorithms

Having learned the characteristics of your data, the next step is to assemble a framework in which you can test the performance of various missing data algorithms on simulated data. This can be achieved in one step using `missCompare::impute_simulated()`, but let's look under the hood and see how this function is operating in the background. You can access these functions in case you would like to play with parameters.    

First, `missCompare::simulate()` will create a dataset that resembles your original dataframe, but with no missing data. The dimensions of the simulated dataframe and all correlations between variables will match the original data - essentially, the only major difference between the simulated dataset and your original data is that here, all variables are normally distributed with a custom set mean and standard deviation. Let's observe:

```{r eval = FALSE} 
simulated <- missCompare::simulate(rownum = metadata$Rows,
                                   colnum = metadata$Columns,
                                   cormat = metadata$Corr_matrix,
                                   meanval = 0,
                                   sdval = 1)
```

Note how the first three arguments are elements of the output list (`metadata`) from the previous function `missCompare::get_data()`.

The output `simulated` is a list, where the first item is the simulated matrix and the next two objects are samples from the correlation matrix of the original and the simulated data.

With this done, missing data can be spiked in in the simulated data using various patterns (MCAR, MAR, MNAR and MAP). This will be done with respect to the missing data fraction for each variable in the original dataframe.

MCAR missingness is simple, NAs are spiked in randomly:
```{r eval = FALSE} 
missCompare::MCAR(simulated$Simulated_matrix,
                  missfrac_per_var = metadata$Fraction_missingness_per_variable)
```

MAR and MNAR missingness follow similar logic. With MNAR, NAs are spiked in based on the variable's own values, while with MAR, NAs are spiked in based on another variable's values (univariate). In both cases, a window defines how extreme the missing data pattern will be. With smaller windows, data will be set to missing from values closer to each other - for more information, there is a detailed description at `?MAR` and `?MNAR`.
```{r eval = FALSE} 
missCompare::MAR(simulated$Simulated_matrix,
                 missfrac_per_var = metadata$Fraction_missingness_per_variable,
                 window = 0.5)

missCompare::MNAR(simulated$Simulated_matrix,
                  missfrac_per_var = metadata$Fraction_missingness_per_variable,
                  window = 0.5)
```

MAP missingness is a possible combination of any of the previous three missing data pattern. You can define what pattern you assume per variable. In our case, let's say that all variables are missing randomly, except for education (the last variable), which is missing not at random. This can be defined as:
```{r eval = FALSE} 
missCompare::MAP(simulated$Simulated_matrix,
                 missfrac_per_var = metadata$Fraction_missingness_per_variable,
                 assumed_pattern = c(rep("MCAR", 10), "MNAR"),
                 window = 0.5)
```

All patterns can be run with a single line using `missCompare::all_patterns()`.

Again, `missCompare::impute_simulated()does all these in one step, and more:     
1. it simulates the dataset with no missing values;     
2. spikes in NAs in MCAR, MAR, MNAR and - if you define a custom pattern - MAP patterns;     
3. imputes missing data using a curated list of 16 algorithms;     
4. evaluates imputation accuracy by calculating RMSE values between the imputed and the original datapoints.

This can be achieved by a single command! This means you DON'T have to run `missCompare::simulate()`, `missCompare::MCAR()`, `missCompare::MAR()`, `missCompare::MNAR()`, `missCompare::MAP()` or `missCompare::all_patterns()`. The command goes as (note that we defined `assumed_pattern = NA` - the defeault - so MAP pattern will be skipped):

```{r eval = FALSE} 
missCompare::impute_simulated(rownum = metadata$Rows,
                              colnum = metadata$Columns, 
                              cormat = metadata$Corr_matrix, 
                              missfrac_per_var = metadata$Fraction_missingness_per_variable,
                              n.iter = 50, 
                              assumed_pattern = NA, 
                              window = 0.5)
```

The `n.iter` argument controls how many times the process will repeat itself (spiking in missing data, imputation and assessment of imputation accuracy, that is).

When this is completed (it can take some time!), you should get messages stating what are the best performing algorithms for each missingness patterns and a graph that gives you a visual aid on which algorithms perform with the smallest imputation errors. The less the error, the better the imputation accuracy!

```{r echo=FALSE, fig.cap=" ", out.width = '80%'}
knitr::include_graphics("/Users/med-tv_/Documents/GitHub/missCompare/vignettes/RMSE_plot.pdf")
```

## Validating best performing algorithms

From the previous analysis, we learned that **missForest** performs the best with MCAR and MAR patterns, while the **pcaMethods BPCA** method is best performing with MNAR pattern. It is also obvious from the graph and the output statistics that **Random replacement** performs with large errors.

In the next step, you can revert to using your own data and validate these results using `missCompare::impute_data_validate()`. For this, you will need to spike in a very tiny fraction of random missing data in your data. You don't want to over-do the missing data spike-in, as your data already contains missing datapoints, so around 1\%-2\% will do (define this using `spike.in`). 

When defining the validation code, you will have the opportunity to scale and center your data or leave it in its native distribution (define this using `scale`) - while scaling is a must for distance-based methods (such as pcaMethods BPCA or the rest of the PCA methods), such transformation might not be necessary for missForest, for example.

Also, while some of the methods allow factor variables (e.g. missForest or mice), some will not (e.g. PCA based methods) - it is your task here to make sure that the data that you input the validation algorithm is appropriate and clean. 

When defining the codes here, you can select which algorithms you would like to validate (by defining the argument `sel_method`, you can select all of them, only one of them, or any combinations - see `?impute_data_validate` for numeric coding of methods) and how many interations you would like to run (by defining `n.iter`).

If you want, you can also do this validation step in two separate blocks - one for methods that require scaling, one for those without this, etc..

Now, back to our data - as we observed previously, **missForest** and **pcaMethods BPCA** performed the best, while **Random replacement** performed the worst. Let's use all algorithms in the validation step and see whether the results here parallel what we obtained using the simulated data in the previous step:

```{r eval = FALSE} 
missCompare::impute_data_validate(cleaned$Dataframe_clean,
                                  scale = T,
                                  spike.in = 0.01,
                                  n.iter = 5,
                                  sel_method = c(1:16))
```

The plot we obtain looks like this:

```{r echo=FALSE, fig.cap=" ", out.width = '80%'}
knitr::include_graphics("/Users/med-tv_/Documents/GitHub/missCompare/vignettes/VALID_plot.pdf")
```

Looks like **missForest** and **pcaMethods BPCA** are indeed one of the best performing algorithms for this data.

There will be situations where the results from the simulated dataset and your own data from the validations step do not perfectly align - this can be due to a number of reasons, e.g. the variables in the simulated data are all numeric and normally distributed, whereas this is not the case in your own data, etc.

## Imputing your dataset

Hopefully, by now you have a clear(er) idea on which method you would like to use to impute missing data for your dataset - you will be able to do this using `missCompare::impute_data()`. If you completed the validation step, this will be very easy for you, as the codes are very similar here. If you skipped validation, don't worry, it won't be too hard. Let's consider a scenario where you choose **missForest** to impute your data. You do not want to scale your data and you would like to create 10 imputed dataframes in order to set up a multiple imputation framework. Note, that setting up a multiple imputation framework is only a viable option with probabilistic models (e.g. you can impute you data with **Mean imputation** a billion times, but the NAs will always be replaced with exacty the same mean values!).

```{r eval = FALSE} 
imputed <- missCompare::impute_data(cleaned$Dataframe_clean, 
                         scale = F, 
                         n.iter = 10, 
                         sel_method = c(14)) # 14 is the code for missForest
```

The 10 imputed dataframes will be available under `imputed$missForest_imputation[[1]]`, `imputed$missForest_imputation[[2]]`, `imputed$missForest_imputation[[3]]`, etc..., while all the other elements in the list will be empty (e.g. nothing in `imputed$median_imputation`).

Let's see what happens when you similarly define 10 copies, but choose a non-probabilistic method - **Mean imputation** in this case.

```{r eval = TRUE, message = FALSE} 
imputed <- missCompare::impute_data(cleaned$Dataframe_clean, 
                         scale = T, 
                         n.iter = 10, 
                         sel_method = c(3)) # 3 is the code for mean imputation
```

You will see that the algorithm runs only 1 iteration for **Mean imputation**, and there is only one object, under `imputed$mean_imputation[[1]]`. There are no `imputed$mean_imputation[[2]]`, `imputed$mean_imputation[[3]]`, etc... created at all.

Well done! You have imputed your data. Almost there, only one step remains - to assess how the imputation affected your dataset.

## Post imputation diagnostics

The function `missCompare::post_imp_diag()` will help you assess how the imputation affected important parameters of your data.

Let's run this with the results of the **Mean imputation** algorithm (`imputed$mean_imputation[[1]]`):

```{r eval = TRUE} 
diag <- missCompare::post_imp_diag(cleaned$Dataframe_clean,
                                   imputed$mean_imputation[[1]], 
                                   scale=F, 
                                   n.boot = 5)
```

The `diag` list contains several interesting assessments. The `Densityplots` and `Boxplots` lists should give an idea on the distributions of the original and the imputed values. Perhaps the limitations of mean imputation are the most apparent on these plots. For example, let's inspect HDL levels on this boxplot:

```{r echo = FALSE, fig.width=6, fig.height=3} 
diag$Boxplots$HDL
```

The `Barcharts` list only contains elements if there were factors in the original data. 

The `Statistics` list shows the mean and SD values for the original and imputed values per variables and calculates Welch's t test on the two groups of values.

The `Variable_clusters_orig` and `Variable_clusters_imp` plots visualize the clusters of variables before and after the imputation. The clusters should not change too much!

```{r echo = FALSE, fig.width=6, fig.height=3} 
diag$Variable_clusters_imp
```

Finally, the `Correlation_plot` shows bootstrapped correlation coefficients from the original data and the imputed data. You can specify how many iterations you would like to run to obtain bootstrapped correlation coefficients and 95\% confidence intervals using the `n.boot` argument in the function. In this vignette this is set to 5, which is far too low. The blue line has intercept 0 and slope 1 and  correlation coefficients (represented by the dots and the red line) should align this line as much as possible. With mean and median imputations, the line is often "turned clockwise", with a lower slope than 1 (regression to the mean).

```{r echo = FALSE, fig.width=6, fig.height=3} 
diag$Correlation_plot
```

## Closing words

Well done!
You completed the tutorial and by now, you should ideally have one or more imputed datasets of your data. Awesome job! We hope that you achieved what you were set out to do and you are ready for the next stes in your statistical analysis! Please let us know if you have any issues here -  [missCompare GitHub](https://github.com/Tirgit/missCompare/issues) - or contact the authors if you have any feedback!

 -- authors of missCompare
